{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5810f4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing.ipynb\n",
    "\n",
    "# Step 1: Setup\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "DOCUMENTS_DIR = \"../data/documents\"\n",
    "INDEX_DIR = \"../embeddings/faiss_index\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Or use a GPU-optimized model\n",
    "\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fdcb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Load SentenceTransformer\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "model.to('cuda')  # Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dae114",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: PDF to Text\n",
    "def extract_text_from_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e116f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Chunking and Cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_tokens=256):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks, chunk = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < max_tokens:\n",
    "            chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \". \"\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86550bca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Embed and Store\n",
    "texts, metadata = [], []\n",
    "\n",
    "for filename in os.listdir(DOCUMENTS_DIR):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(f\"Processing {filename}...\")\n",
    "        pdf_path = os.path.join(DOCUMENTS_DIR, filename)\n",
    "        text = clean_text(extract_text_from_pdf(pdf_path))\n",
    "        chunks = chunk_text(text)\n",
    "        for chunk in chunks:\n",
    "            texts.append(chunk)\n",
    "            metadata.append({\"source\": filename})\n",
    "\n",
    "print(f\"Total chunks: {len(texts)}\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True, device='cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7963c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Save FAISS Index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "faiss.write_index(index, os.path.join(INDEX_DIR, \"docs.index\"))\n",
    "with open(os.path.join(INDEX_DIR, \"metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"texts\": texts, \"meta\": metadata}, f)\n",
    "\n",
    "print(\"Indexing complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
